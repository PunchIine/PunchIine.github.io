<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="LAzy"><title>yolo · LAzy</title><meta name="description" content="IoU（Intersection over union）
交并比，衡量两个区域的重叠程度，是二者重叠部分面积占二者总面积的比例


在目标检测任务中，如果我们模型输出的矩形框与我们人工标注的矩形框的IoU值大于某个阈值时（通常为0.5）即认为我们的模型输出了正确的
Precision &amp;amp; R"><meta name="keywords" content="Blog,博客,LAzy"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.webp"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/insight.css"><link rel="stylesheet" href="/css/search.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><meta name="generator" content="Hexo 6.1.0"></head><body><div class="page-top animated fadeInDown"><div class="nav"><li> <a href="/archives">Archives</a></li><li> <a href="/">Home</a></li><li> <a href="/tags">Tags</a></li><li> <a href="/about">About</a></li><li> <a href="/links">Links</a></li></div><div class="information"><div class="nav_right_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li><li><a class="fa fa-search" onclick="openWindow();"></a></li></div><div class="avatar"><img src="https://www.helloimg.com/images/2022/04/08/RsLwlM.jpg"></div></div></div><div class="sidebar animated fadeInDown"><div class="sidebar-top"><div class="logo-title"><div class="title"><img src="https://www.helloimg.com/images/2022/04/08/RsLwlM.jpg" style="width:200px;" alt="favicon"><h3 title=""><a href="/">LAzy</a></h3><div class="description"><p>Learning right now is always the best solution</p></div></div><ul class="social-links"><li><a target="_blank" rel="noopener" href="https://github.com/so1c/notes"><i class="fa fa-github"></i></a></li><li><a href="mailto:811735602@qq.com"><i class="fa fa-envelope"></i></a></li></ul></div></div><div class="footer"><div class="p"> <span> 全站CC-BY-SA-3.0 </span><i class="fa fa-star"></i><span> LAzy</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> & </span><span>Anatolo </span></div><div class="beian"></div></div></div><div class="main"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>yolo</a></h3></div><div class="post-content"><p><!-- TOC -->



<h2><span id="iouintersection-over-union">IoU（<strong>Intersection over union</strong>）</span></h2><blockquote>
<p>交并比，衡量两个区域的重叠程度，是二者重叠部分面积占二者总面积的比例</p>
</blockquote>
<p><img src="https://user-images.githubusercontent.com/93063038/163665429-095df1bd-6ac4-4023-a4e3-b58936722093.png" alt="img"></p>
<p>在目标检测任务中，如果我们模型输出的矩形框与我们人工标注的矩形框的IoU值大于某个阈值时（通常为0.5）即认为我们的模型输出了正确的</p>
<h2><span id="precision-amp-recall"><strong>Precision &amp; Recall</strong></span></h2><p>假设我们有一组图片，里面有若干待检测的目标，</p>
<p>Precision就代表我们模型检测出来的目标有多大比例是真正的目标物体，</p>
<p>Recall就代表所有真实的目标有多大比例被我们的模型检测出来了。</p>
<p><img src="https://user-images.githubusercontent.com/93063038/163666221-257ded9e-5809-4e48-bb2f-abc97f0ce6c7.png" alt="image"></p>
<p><img src="https://user-images.githubusercontent.com/93063038/163666250-5edb4b65-7d64-4cff-a5ed-83492325219d.png" alt="image"></p>
<p>TP ：模型预测为某类物品且与数据集标注中该类物品IOU大于0.5</p>
<p>TN ：模型预测不为某类物品但与数据集标注中该类物品IOU大于0.5</p>
<p>FP ：模型预测为某类物品，但与数据集标注中所有物品（包括非此类）IOU均大于0.5</p>
<p>FN ：模型预测不是任何一类物品，且与数据集中所有物品IOU均大于0.5</p>
<img width="447" alt="image" src="https://user-images.githubusercontent.com/93063038/165037636-4aa10c6c-0368-4fad-9dcd-0c7b4d95578e.png">



<p>准确率： 模型检测出的物品中正确的比例</p>
<p><img src="https://user-images.githubusercontent.com/93063038/163666288-f948f80e-77df-47ab-b850-09f8990e79c1.png" alt="image"></p>
<p>召回率： 所有正确的目标中被模型检测出来的比例</p>
<p><img src="https://user-images.githubusercontent.com/93063038/163666293-016cc94a-35c1-4fc0-9be5-dd7257e4e60a.png" alt="image"></p>
<h2><span id="pr曲线">PR曲线</span></h2><blockquote>
<p>我们当然希望检测的结果P越高越好，R也越高越好，但事实上这两者在<strong>某些情况下是矛盾的</strong>。比如极端情况下，我们只检测出了一个结果，且是准确的，那么Precision就是100%，但是Recall就很低；而如果我们把所有结果都返回，那么必然Recall必然很大，但是Precision很低。</p>
<p>因此在不同的场合中需要自己判断希望P比较高还是R比较高。如果是做实验研究，可以绘制Precision-Recall曲线来帮助分析。</p>
</blockquote>
<img width="544" alt="image" src="https://user-images.githubusercontent.com/93063038/163702132-5b2310db-95e7-459e-af1f-f44cd803eb3e.png">



<h2><span id="ap-average-precision">AP (Average Precision)</span></h2><p>AP &#x3D; $\int^1_0p(r)dr$</p>
<p>在实际应用中，我们并不直接对该PR曲线进行计算，而是对PR曲线进行平滑处理。即对PR曲线上的每个点，Precision的值取该点右侧最大的Precision的值。</p>
<p>？</p>
<h1><span id="yolov1">Yolov1</span></h1><blockquote>
<p><strong>将目标检测问题转化为一个回归问题进行求解</strong>，也就是说将图像作为输入（像素数据），直接输出物体的位置和所属于的类别的置信度（是以一个向量的形式表示的，后续会介绍），属于端到端的模型形式。</p>
</blockquote>
<p>基本思想：将图片划分为S*S个区域（gridcell），假设都存在一个 true answer 也就是针对这个目标的最好的检测框 ， 则每一个目标的检测框的中心点一定是落在某一个小区域内的；如果此时的中心点落在 x 框内，则 x 小区域就负责搞定这个目标；注意此时可能多个目标落在同一个区域内。</p>
<p>每一个<strong>小区域设定 B （bounding box的数量）个可能的候选框，并计算每一个可能的候选框的得分</strong> &#x3D; 置信度，是一个（该候选框和真实的目标检测框的重合程度）和（这个框里确实框住了某一个物体）的综合度量指标，计算方式如下：</p>
<p>$confidence &#x3D; Pr（Object）* IOU^{truth}_{pred}$</p>
<p><strong>其中，若bounding box包含物体，则P(object) &#x3D; 1；否则P(object) &#x3D; 0</strong></p>
<p>每一个预测是一个长度为 5 的向量，记作 （x, y, w, h, conf）</p>
<ul>
<li>(x, y) 表示当前预测的检测框的中心相对于我的小区域的位置（共 <img src="https://www.zhihu.com/equation?tex=S%5E2" alt="[公式]"> 个小区域)，这里的 x 和 y 都是 0-1 之间的，也就是说是相对于当前小区域的左上角的偏移值</li>
<li>(w,h) 表示检测框的宽度和高度，一般是处理到 0-1 之间，标记当前的预测框和整个图片的宽度&#x2F;高度的比例 - conf 为上述的置信度，可以看作是当前的框的可信度的综合指标，由（是否框准了 &#x3D; 是否和真实的预测框有较好的重合）和（是否框里确实框住了物体）两个部分影响</li>
</ul>
<p>网络结构</p>
<blockquote>
<p>Our detection network has 24 convolutional layers followed by 2 fully connected layers. Alternating 1 <em>×</em> 1 convolutional layers reduce the features space from preceding layers. We pretrain the convolutional layers on the ImageNet classifification task at half the resolution (224 <em>×</em> 224 input image) and then double the resolution for detection.</p>
</blockquote>
<p>YOLO网络借鉴了GoogLeNet分类网络结构。 网络有24个卷积层，其后是2个完全连接的层，不同的是，YOLO未使用inception module，而是使用1x1卷积层（此处1x1卷积层的存在是为了跨通道信息整合）+3x3卷积层简单替代。最终输出的是7x7x30的张量的预测值</p>
<img width="665" alt="image" src="https://user-images.githubusercontent.com/93063038/165040258-989f9ec2-36c5-411a-9ef7-ec396115ab5a.png">







<p>损失函数</p>
<p>使用<strong>均方和误差</strong>作为loss函数来优化模型参数，即网络输出的SxSx(Bx5 + C)维向量与真实图像的对应SxSx(Bx5 + C)维向量的均方和误差。</p>
<img width="523" alt="image" src="https://user-images.githubusercontent.com/93063038/165040626-b6c30262-1151-4de8-92e7-413f53811f33.png">



<p>NMS 方法（Non-Maximal Suppression &#x2F; 非极大值抑制）</p>
<blockquote>
<p>将同一目标内的bboxes按照cls score + IoU阈值做筛选，剔除冗余地、低置信度的bbox</p>
</blockquote>
<h1><span id="yolov3">Yolov3</span></h1><p>实际框    预测框  anchor box</p>
<h2><span id="anchor-box">Anchor Box</span></h2><p>得到特征图 S×S×(5+C)</p>
</p><div class="tip">本文采用CC-BY-SA-3.0协议，转载请注明出处<br>Author: LAzy</div></div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2022-04-27</span><i class="fa fa-tag"></i><span class="leancloud_visitors"></span><span>About 1404 words, 4 min 40 sec  read</span></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="" onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,http://LAzy.github.io/2022/04/27/yolo/,LAzy,yolo,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2022/04/27/baseline/" title="baseline">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2022/04/27/OpenMV%E5%AE%9E%E7%8E%B0%E9%9D%B6%E7%82%B9%E6%A3%80%E6%B5%8B/" title="OpenMV实现靶点检测">Next</a></li></ul></div><script src="/js/visitors.js"></script></div></div></div></div><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script><script>(function(window){var INSIGHT_CONFIG={TRANSLATION:{POSTS:"Posts",PAGES:"Pages",CATEGORIES:"Categories",TAGS:"Tags",UNTITLED:"(Untitled)",},CONTENT_URL:"/content.json",};window.INSIGHT_CONFIG=INSIGHT_CONFIG})(window);</script><script src="/js/insight.js" defer></script><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="Search..."><span class="searchbox-close"><a class="fa fa-times-circle" onclick="closeWindow();"></a></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"><p>Seraching...</p></div></div></div></div></body></html>