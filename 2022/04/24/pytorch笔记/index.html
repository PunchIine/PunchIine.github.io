<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="LAzy">
    <meta name="referrer" content="no-referrer" />
    
    <title>
        
            pytorch笔记 |
        
        LAzy&#39;s Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="https://www.helloimg.com/images/2022/04/08/RsLwlM.jpg">
    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"lazy.github.io","root":"/","language":"en","path":"search.json"};
    KEEP.theme_config = {"toc":{"enable":true,"number":false,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"https://www.helloimg.com/images/2022/04/08/RsLwlM.jpg","favicon":"https://www.helloimg.com/images/2022/04/08/RsLwlM.jpg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":false,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving."},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":true}}},"local_search":{"enable":false,"preload":false},"code_copy":{"enable":false,"style":"default"},"pjax":{"enable":false},"lazyload":{"enable":false},"version":"3.4.5"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>
<meta name="generator" content="Hexo 6.1.0"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
                <a class="logo-image" href="/">
                    <img src="https://www.helloimg.com/images/2022/04/08/RsLwlM.jpg">
                </a>
            
            <a class="logo-title" href="/">
                LAzy&#39;s Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/categories"
                            >
                                CATEGORIES
                            </a>
                        </li>
                    
                    
                </ul>
            </div>
            <div class="mobile">
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/categories">CATEGORIES</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">pytorch笔记</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="https://www.helloimg.com/images/2022/04/08/RsLwlM.jpg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">LAzy</span>
                        
                            <span class="author-label">Lv2</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;
        <span class="pc">2022-04-24 15:01:09</span>
        <span class="mobile">2022-04-24 15:01</span>
    </span>
    
    

    
    
    
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <h3 id="一-张量"><a href="#一-张量" class="headerlink" title="一.张量"></a>一.张量</h3><h4 id="1-张量的数据类型"><a href="#1-张量的数据类型" class="headerlink" title="1.张量的数据类型"></a>1.张量的数据类型</h4><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202201311115752.png" alt="image-20220131111457646"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202201311137743.png" alt="image-20220131113706683"></p>
<p>默认数据类型为32位浮点型</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202201311137323.png" alt="image-20220131113746252"></p>
<p><strong>torch.set_default_tensor_type()</strong>函数可设置默认的张量数据类型</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202201311141394.png" alt="image-20220131114156317"></p>
<p><strong>a.long()  a.int()  a.float()方法</strong></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202201311143047.png" alt="image-20220131114336959"></p>
<h4 id="2-张量的生成"><a href="#2-张量的生成" class="headerlink" title="2.张量的生成"></a>2.张量的生成</h4><p>(1)列表或序列可通过<strong>torch.tensor()</strong>函数构造张量</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202011519786.png" alt="image-20220201151919676"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202201311204496.png" alt="image-20220131120443462"></p>
<p><strong>.shape .size .numel()</strong>方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a  = torch.randn((<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>),dtype = torch.float32)</span><br><span class="line"><span class="built_in">print</span>(a.shape)<span class="comment">#打印“torch.Size([2,3,5])”不需要加括号，直接访问成员属性，返回的是torch.Size类对象，</span></span><br><span class="line"><span class="built_in">print</span>(a.shape[<span class="number">1</span>])<span class="comment">#可以使用[]索引访问,所以size属性是一个迭代器</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a.size())<span class="comment">#打印“torch.Size([2,3,5])”,与shape属性一致</span></span><br><span class="line"><span class="built_in">print</span>(a.size(<span class="number">1</span>))<span class="comment">#可传入参数，返回3，即第i维的个数</span></span><br><span class="line"><span class="built_in">print</span>(a.numel)<span class="comment">#返回30，计算张量中包含元素数量</span></span><br></pre></td></tr></table></figure>
<p>通过<strong>torch.tensor()</strong>函数构造张量可使用<strong>dtype</strong>参数指定数据类型，使用requires_grad来指定是否需要计算梯度</p>
<p>(2)<strong>torch.Tensor()</strong>——————一个类</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202011523056.png" alt="image-20220201152349906"></p>
<p>可以生成指定形状的张量</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202011543977.png" alt="image-20220201154329908"></p>
<p>(3)<strong>torch.from_numpy(ndarray)</strong>  <strong>torch.as_tensor()</strong></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202011545516.png" alt="image-20220201154511394"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202011603578.png" alt="image-20220201160311505"></p>
<p>(4)依据数值创建</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202011548415.png" alt="image-20220201154817290"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202011549781.png" alt="image-20220201154949675"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202011552566.png" alt="image-20220201155232420"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202011553085.png" alt="image-20220201155307992"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202011554567.png" alt="image-20220201155410450"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202011554715.png" alt="image-20220201155438613"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202011555299.png" alt="image-20220201155534168"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202011555716.png" alt="image-20220201155557606"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202020945847.png" alt="image-20220202094539781"></p>
<p><strong>torch.empty()</strong>————返回填充有未初始化数据的张量，张量的形状由可变的参数大小定义</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.empty(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">tensor(<span class="number">1.00000e-08</span> *</span><br><span class="line">       [[ <span class="number">6.3984</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>]])</span><br></pre></td></tr></table></figure>
<p>(5)依据概率分布创建张量</p>
<p><strong>torch.manual_seed()</strong>——————指定生成随机数种子</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202011604558.png" alt="image-20220201160448443"></p>
<p> <img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202011614983.png" alt="image-20220201161451904"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202011615034.png" alt="image-20220201161514884"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202011616898.png" alt="image-20220201161608761"></p>
<h4 id="3-张量的操作"><a href="#3-张量的操作" class="headerlink" title="3.张量的操作"></a>3.张量的操作</h4><h5 id="（1）张量的拼接与切分"><a href="#（1）张量的拼接与切分" class="headerlink" title="（1）张量的拼接与切分"></a>（1）张量的拼接与切分</h5><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202021015601.png" alt="image-20220202101504497"></p>
<p><strong>torch.stack()</strong>会拓展张量维度</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202021018613.png" alt="image-20220202101844509"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202021020023.png" alt="image-20220202102018924"></p>
<h5 id="（2）张量索引"><a href="#（2）张量索引" class="headerlink" title="（2）张量索引"></a>（2）张量索引</h5><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202021023763.png" alt="image-20220202102350596"></p>
<p>==注意index参数的数据类型必须是torch.long==</p>
<p>例</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202021030467.png" alt="image-20220202103022392"></p>
<p>(对第0维度进行索引（相当于索引第一维度）)</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202021032248.png" alt="image-20220202103238169"></p>
<h5 id="（3）张量变换"><a href="#（3）张量变换" class="headerlink" title="（3）张量变换"></a>（3）张量变换</h5><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202021034687.png" alt="image-20220202103425616"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202041220593.png" alt="image-20220204122005493"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202041236220.png" alt="image-20220204123625169"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202041234792.png" alt="image-20220204123440698"></p>
<p>==注意reshape共享数据内存==</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202021037649.png" alt="image-20220202103706542"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202021038823.png" alt="image-20220202103810700"></p>
<p>如图：</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202021039152.png" alt="image-20220202103951071"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202041702785.png" alt="image-20220204170210714"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202041707930.png" alt="image-20220204170737883"></p>
<p>沿着指定的维度重复tensor。不同与expand()，本函数复制的是tensor中的数据。扩展（expand）张量不会分配新的内存，只是在存在的张量上创建一个新的视图（view），一个大小（size）等于1的维度扩展到更大的尺寸。repeat沿着特定的维度重复这个张量，和expand()不同的是，这个函数拷贝张量的数据。</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202041716152.png" alt="image-20220204171657106"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202041717585.png" alt="image-20220204171731493"></p>
<h5 id="（4）张量数学运算"><a href="#（4）张量数学运算" class="headerlink" title="（4）张量数学运算"></a>（4）张量数学运算</h5><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202021045502.png" alt="image-20220202104524351"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202021044623.png" alt="image-20220202104455497"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202041733245.png" alt="image-20220204173330127"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202041913380.png" alt="image-20220204191336309"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202041914954.png" alt="image-20220204191459872"></p>
<p>真多啊。。。用到再查叭。</p>
<h3 id="二-pytorch中的自动求导"><a href="#二-pytorch中的自动求导" class="headerlink" title="二.pytorch中的自动求导"></a>二.pytorch中的自动求导</h3><p>将张量的<strong>requires_grad参数</strong>设为Ture可自动求导得到其梯度</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202042022817.png" alt="image-20220204202233657"></p>
<p><strong>Tensor()类的重要属性：</strong></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202042036027.png" alt="image-20220204203646918"></p>
<p>在Pytorch中，默认情况下，非叶节点的梯度值在反向传播过程中使用完后就会被清除，不会被保留。<strong>只有叶子节点的梯度值能够被保留下来</strong></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202042032787.png" alt="image-20220204203208599"></p>
<h4 id="retain-grad"><a href="#retain-grad" class="headerlink" title="retain_grad()"></a>retain_grad()</h4><p>可保存非叶子节点梯度</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202042033410.png" alt="image-20220204203319272"></p>
<p>grad_fn：记录创建该张量时所用的方法（函数）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;grad_fn:&quot;</span>, w.grad_fn, x.grad_fn, a.grad_fn, b.grad_fn, y.grad_fn)</span><br><span class="line"><span class="comment"># Out：grad_fn: None None &lt;AddBackward0 object at 0x000001C04BB24788&gt; &lt;AddBackward0 object at 0x000001C04605D188&gt; &lt;MulBackward0 object at 0x000001C04605D1C8&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="autograd"><a href="#autograd" class="headerlink" title="autograd"></a>autograd</h4><h5 id="torch-autograd-backward"><a href="#torch-autograd-backward" class="headerlink" title="torch.autograd.backward"></a>torch.autograd.backward</h5><p>张量中的backward()方法直接调用了torch.autograd.backward()</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202042048465.png" alt="image-20220204204833276"></p>
<p><strong>retain_graph</strong>参数设置为True，得以进行两次反向传播</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202051129435.png" alt="image-20220205112941344"></p>
<p><strong>grad_tensors</strong>参数用于多梯度权重的设置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">w = torch.tensor([<span class="number">1.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">    x = torch.tensor([<span class="number">2.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    a = torch.add(w, x)     <span class="comment"># retain_grad()</span></span><br><span class="line">    b = torch.add(w, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    y0 = torch.mul(a, b)    <span class="comment"># y0 = (x+w) * (w+1)</span></span><br><span class="line">    y1 = torch.add(a, b)    <span class="comment"># y1 = (x+w) + (w+1)    dy1/dw = 2</span></span><br><span class="line"></span><br><span class="line">    loss = torch.cat([y0, y1], dim=<span class="number">0</span>)       <span class="comment"># [y0, y1]</span></span><br><span class="line">    grad_tensors = torch.tensor([<span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line">    <span class="comment"># grad_tensors = torch.tensor([1., 2.])</span></span><br><span class="line"></span><br><span class="line">    loss.backward(gradient=grad_tensors)    <span class="comment"># gradient 传入 torch.autograd.backward()中的grad_tensors</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(w.grad)</span><br><span class="line">    </span><br><span class="line">    out：<span class="number">7</span> <span class="comment"># 9</span></span><br></pre></td></tr></table></figure>
<h5 id="torch-autograd-grad"><a href="#torch-autograd-grad" class="headerlink" title="torch.autograd.grad"></a>torch.autograd.grad</h5><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202042059611.png" alt="image-20220204205947396"></p>
<p><strong>create_graph</strong>   创建导数计算图，用于高阶求导</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202051145014.png" alt="image-20220205114554928"></p>
<h5 id="autograd小贴士："><a href="#autograd小贴士：" class="headerlink" title="autograd小贴士："></a>autograd小贴士：</h5><p>1.梯度不自动清零</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202051150324.png" alt="image-20220205115044251"></p>
<p>使用<strong>grad.zero_()</strong>  对梯度清零</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202051151087.png" alt="image-20220205115154019"></p>
<p>2.依赖于叶子节点的节点， requires_grad默认为True</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202051155322.png" alt="image-20220205115547253"></p>
<p>3.叶子节点不可执行in-place操作</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202051200092.png" alt="image-20220205120026028"></p>
<p>可知+=操作时不改变内存地址，为in-place操作，不可对叶子节点执行</p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202051202037.png" alt="image-20220205120213991"></p>
<h3 id="三-torch-nn模块"><a href="#三-torch-nn模块" class="headerlink" title="三.torch.nn模块"></a>三.torch.nn模块</h3><h4 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h4><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190444179.png" alt="image-20220219044440063"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190444944.png" alt="image-20220219044452816"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190448651.png" alt="image-20220219044809497"></p>
<h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202051346913.png" alt="image-20220205134635851"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以torch.nn.Conv2d()为例， 介绍卷积再图像上的使用方法，其调用方式为：</span></span><br><span class="line">torch.nn.Conv2d(in_channels,</span><br><span class="line">               out_channels,</span><br><span class="line">               kernel_size,</span><br><span class="line">               stride=<span class="number">1</span>,</span><br><span class="line">               padding=<span class="number">0</span>,</span><br><span class="line">               dilation=<span class="number">1</span>,</span><br><span class="line">               groups=<span class="number">1</span>,</span><br><span class="line">               bias=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202051349651.png" alt="image-20220205134953580"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">myim = Image.<span class="built_in">open</span>(<span class="string">&quot;lenna.jpg&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">myimgray = np.array(myim.convert(<span class="string">&quot;L&quot;</span>), dtype=np.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">imh, imw = myimgray.shape</span><br><span class="line">myimgray_t = torch.from_numpy(myimgray.reshape((<span class="number">1</span>, <span class="number">1</span>, imh, imw)))</span><br><span class="line"></span><br><span class="line">kersize = <span class="number">5</span></span><br><span class="line">ker = torch.ones(kersize, kersize, dtype=torch.float32)*-<span class="number">1</span></span><br><span class="line">ker[<span class="number">2</span>, <span class="number">2</span>] = <span class="number">24</span></span><br><span class="line">ker = ker.reshape((<span class="number">1</span>, <span class="number">1</span>, kersize, kersize))</span><br><span class="line"></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">2</span>, (kersize, kersize), bias = <span class="literal">False</span>)</span><br><span class="line">conv2d.weight.data[<span class="number">0</span>] = ker</span><br><span class="line">imconv2dout = conv2d(myimgray_t)</span><br><span class="line">imconv2dout_im = imconv2dout.data.squeeze()</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">plt.imshow(imconv2dout_im[<span class="number">0</span>], cmap=plt.cm.gray)</span><br><span class="line">plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202070301469.png" alt="image-20220207030119312"></p>
<p><strong>可见使用边缘特征提取卷积核很好的提取出了图像的边缘信息</strong></p>
<h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202070317522.png" alt="image-20220207031712430"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202070317316.png" alt="image-20220207031744256"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202130929419.png" alt="image-20220213092921319"></p>
<h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202130930276.png" alt="image-20220213093044229"></p>
<h4 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h4><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202130950880.png" alt="image-20220213095021812"></p>
<h3 id="四-pytorch中的数据操作与预处理"><a href="#四-pytorch中的数据操作与预处理" class="headerlink" title="四.pytorch中的数据操作与预处理"></a>四.pytorch中的数据操作与预处理</h3><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202130951963.png" alt="image-20220213095134915"></p>
<h4 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h4><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190325547.png" alt="image-20220219032522337"></p>
<h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h4><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190326166.png" alt="image-20220219032624056"></p>
<h4 id="transforms"><a href="#transforms" class="headerlink" title="transforms"></a>transforms</h4><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190354368.png" alt="image-20220219035439267"></p>
<h4 id="crop"><a href="#crop" class="headerlink" title="crop"></a>crop</h4><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190357254.png" alt="image-20220219035758981"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190402535.png" alt="image-20220219040238390"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190407217.png" alt="image-20220219040747063"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190407901.png" alt="image-20220219040715776"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190409623.png" alt="image-20220219040934500"></p>
<h4 id="flip"><a href="#flip" class="headerlink" title="flip"></a>flip</h4><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190413544.png" alt="image-20220219041354431"></p>
<h4 id="rotation"><a href="#rotation" class="headerlink" title="rotation"></a>rotation</h4><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190417363.png" alt="image-20220219041707239"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190425111.png" alt="image-20220219042546974"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190426940.png" alt="image-20220219042619787"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190427628.png" alt="image-20220219042739525"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190428019.png" alt="image-20220219042807856"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190428461.png" alt="image-20220219042851339"></p>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190430602.png" alt="image-20220219043053494"></p>
<h4 id="transforms的操作"><a href="#transforms的操作" class="headerlink" title="transforms的操作"></a>transforms的操作</h4><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190431831.png" alt="image-20220219043148686"></p>
<h4 id="自定义transforms"><a href="#自定义transforms" class="headerlink" title="自定义transforms"></a>自定义transforms</h4><p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202190434716.png" alt="image-20220219043402594"></p>
<h3 id="正态分布与平方损失"><a href="#正态分布与平方损失" class="headerlink" title="正态分布与平方损失"></a>正态分布与平方损失</h3><p>接下来，我们通过对噪声分布的假设来解读平方损失目标函数。</p>
<p>正态分布和线性回归之间的关系很密切。 正态分布（normal distribution），也称为<em>高斯分布</em>（Gaussian distribution）， 最早由德国数学家高斯（Gauss）应用于天文学研究。 简单的说，若随机变量xx具有均值μμ和方差σ2σ2（标准差σσ），其正态分布概率密度函数如下：</p>
<p>(3.1.11)</p>
<p>p(x)=12πσ2−−−−√exp(−12σ2(x−μ)2).p(x)=12πσ2exp⁡(−12σ2(x−μ)2).</p>
<p>下面我们定义一个Python函数来计算正态分布。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def normal(x, mu, sigma):</span><br><span class="line">    p = 1 / math.sqrt(2 * math.pi * sigma**2)</span><br><span class="line">    return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)</span><br></pre></td></tr></table></figure>
<p>我们现在可视化正态分布。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 再次使用numpy进行可视化</span><br><span class="line">x = np.arange(-7, 7, 0.01)</span><br><span class="line"></span><br><span class="line"># 均值和标准差对</span><br><span class="line">params = [(0, 1), (0, 2), (3, 1)]</span><br><span class="line">d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel=&#x27;x&#x27;,</span><br><span class="line">         ylabel=&#x27;p(x)&#x27;, figsize=(4.5, 2.5),</span><br><span class="line">         legend=[f&#x27;mean &#123;mu&#125;, std &#123;sigma&#125;&#x27; for mu, sigma in params])</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/chen-zeyu0/wohaoxiangshuijiao/raw/master/img/202202231627780.png" alt="image-20220223162721741"></p>
<p>就像我们所看到的，改变均值会产生沿xx轴的偏移，增加方差将会分散分布、降低其峰值。</p>
<p>均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是： 我们假设了观测中包含噪声，其中噪声服从正态分布。 噪声正态分布如下式:</p>
<p>(3.1.12)</p>
<p>y=w⊤x+b+ϵ,y=w⊤x+b+ϵ,</p>
<p>其中，ϵ∼N(0,σ2)ϵ∼N(0,σ2)。</p>
<p>因此，我们现在可以写出通过给定的xx观测到特定yy的<em>似然</em>（likelihood）：</p>
<p>(3.1.13)</p>
<p>P(y∣x)=12πσ2−−−−√exp(−12σ2(y−w⊤x−b)2).P(y∣x)=12πσ2exp⁡(−12σ2(y−w⊤x−b)2).</p>
<p>现在，根据极大似然估计法，参数ww和bb的最优值是使整个数据集的<em>似然</em>最大的值：</p>
<p>(3.1.14)</p>
<p>P(y∣X)=∏i=1np(y(i)|x(i)).P(y∣X)=∏i=1np(y(i)|x(i)).</p>
<p>根据极大似然估计法选择的估计量称为<em>极大似然估计量</em>。 虽然使许多指数函数的乘积最大化看起来很困难， 但是我们可以在不改变目标的前提下，通过最大化似然对数来简化。 由于历史原因，优化通常是说最小化而不是最大化。 我们可以改为<em>最小化负对数似然</em>−logP(y∣X)−log⁡P(y∣X)。 由此可以得到的数学公式是：</p>
<p>(3.1.15)</p>
<p>−logP(y∣X)=∑i=1n12log(2πσ2)+12σ2(y(i)−w⊤x(i)−b)2.−log⁡P(y∣X)=∑i=1n12log⁡(2πσ2)+12σ2(y(i)−w⊤x(i)−b)2.</p>
<p>现在我们只需要假设σσ是某个固定常数就可以忽略第一项， 因为第一项不依赖于ww和bb。 现在第二项除了常数1σ21σ2外，其余部分和前面介绍的均方误差是一样的。 幸运的是，上面式子的解并不依赖于σσ。 因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。</p>

        </div>

        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/2022/04/27/OpenMV%E5%AE%9E%E7%8E%B0%E9%9D%B6%E7%82%B9%E6%A3%80%E6%B5%8B/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">OpenMV实现靶点检测</span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/2022/04/16/jetson%E9%85%8D%E7%BD%AE%E6%97%A5%E5%BF%97/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">jetson配置日志</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2022&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">LAzy</a>
        </div>
        
        <div class="theme-info info-item">
            Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a>
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80-%E5%BC%A0%E9%87%8F"><span class="nav-text">一.张量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%BC%A0%E9%87%8F%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="nav-text">1.张量的数据类型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%BC%A0%E9%87%8F%E7%9A%84%E7%94%9F%E6%88%90"><span class="nav-text">2.张量的生成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E5%BC%A0%E9%87%8F%E7%9A%84%E6%93%8D%E4%BD%9C"><span class="nav-text">3.张量的操作</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%EF%BC%881%EF%BC%89%E5%BC%A0%E9%87%8F%E7%9A%84%E6%8B%BC%E6%8E%A5%E4%B8%8E%E5%88%87%E5%88%86"><span class="nav-text">（1）张量的拼接与切分</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%EF%BC%882%EF%BC%89%E5%BC%A0%E9%87%8F%E7%B4%A2%E5%BC%95"><span class="nav-text">（2）张量索引</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%EF%BC%883%EF%BC%89%E5%BC%A0%E9%87%8F%E5%8F%98%E6%8D%A2"><span class="nav-text">（3）张量变换</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%EF%BC%884%EF%BC%89%E5%BC%A0%E9%87%8F%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="nav-text">（4）张量数学运算</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C-pytorch%E4%B8%AD%E7%9A%84%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC"><span class="nav-text">二.pytorch中的自动求导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#retain-grad"><span class="nav-text">retain_grad()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#autograd"><span class="nav-text">autograd</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#torch-autograd-backward"><span class="nav-text">torch.autograd.backward</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#torch-autograd-grad"><span class="nav-text">torch.autograd.grad</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#autograd%E5%B0%8F%E8%B4%B4%E5%A3%AB%EF%BC%9A"><span class="nav-text">autograd小贴士：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%89-torch-nn%E6%A8%A1%E5%9D%97"><span class="nav-text">三.torch.nn模块</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%B9%E5%99%A8"><span class="nav-text">容器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-text">卷积层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-text">池化层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-text">激活函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82"><span class="nav-text">全连接层</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9B-pytorch%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C%E4%B8%8E%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-text">四.pytorch中的数据操作与预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Dataloader"><span class="nav-text">Dataloader</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dataset"><span class="nav-text">Dataset</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#transforms"><span class="nav-text">transforms</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#crop"><span class="nav-text">crop</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#flip"><span class="nav-text">flip</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#rotation"><span class="nav-text">rotation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#transforms%E7%9A%84%E6%93%8D%E4%BD%9C"><span class="nav-text">transforms的操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89transforms"><span class="nav-text">自定义transforms</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E4%B8%8E%E5%B9%B3%E6%96%B9%E6%8D%9F%E5%A4%B1"><span class="nav-text">正态分布与平方损失</span></a></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>








<div class="post-scripts">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>
